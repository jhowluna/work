{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD_RSER5Lkb5"
   },
   "source": [
    "# Projeto 2: Classificação binária brest cancer com validação cruzada e dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tP2BcEILoLB"
   },
   "source": [
    "## Etapa 1: Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "PGW4fwZmkw6l",
    "outputId": "720945ff-a87a-485f-91bb-839f79566576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skorch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/1e/cc4e1f23cd1faab06672f309e0857294aaa80c5f84670f4d3d19b08ab10b/skorch-0.7.0-py3-none-any.whl (105kB)\n",
      "\r",
      "\u001b[K     |███                             | 10kB 19.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n",
      "Installing collected packages: skorch\n",
      "Successfully installed skorch-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:30.370108Z",
     "start_time": "2020-10-07T23:41:30.361280Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "Yf0FpJ35Lf-Z",
    "outputId": "bab2a47c-5f64-4ae0-a200-7e5ae23ad169"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn        \n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "torch.__version__\n",
    "#pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0SD4dJ4MDMN"
   },
   "source": [
    "## Etapa 2: Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:31.575214Z",
     "start_time": "2020-10-07T23:41:31.565017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "J9aIu62WMGo8",
    "outputId": "5a969a3c-948a-424d-c2df-6e6c0d1c780d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2d2f402230>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:32.014512Z",
     "start_time": "2020-10-07T23:41:32.000494Z"
    },
    "id": "u49yuDE9MJs6"
   },
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('../Bases/entradas_breast.csv')\n",
    "classe = pd.read_csv('../Bases/saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:32.742234Z",
     "start_time": "2020-10-07T23:41:32.554387Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "xXo_IgFGNYwL",
    "outputId": "2fc779ac-db47-436c-dea5-809994e0c5a5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(classe['0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:33.217939Z",
     "start_time": "2020-10-07T23:41:33.210922Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DebmQJZD5Z3D",
    "outputId": "d22e3634-7b4f-4f31-c781-776c15d694af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:33.767997Z",
     "start_time": "2020-10-07T23:41:33.764127Z"
    },
    "id": "flv2JNM05QUf"
   },
   "outputs": [],
   "source": [
    "previsores = np.array(previsores, dtype='float32')\n",
    "classe = np.array(classe, dtype='float32').squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:34.250709Z",
     "start_time": "2020-10-07T23:41:34.242368Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "CzHRBmcs5fPH",
    "outputId": "99ac8dc6-1fcd-4790-c114-a510290bd553"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:34.672224Z",
     "start_time": "2020-10-07T23:41:34.664037Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qbU8a3-Z5ixA",
    "outputId": "429b753f-3034-47d2-c75c-6d0ee50dceb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:35.141237Z",
     "start_time": "2020-10-07T23:41:35.131755Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HyVP8xUi5k-Q",
    "outputId": "b82ac8c1-de5f-4e86-d7fd-5e66ff2fa4d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MI0xnzWjSJg"
   },
   "source": [
    "## Etapa 3: Classe para estrutura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:36.492731Z",
     "start_time": "2020-10-07T23:41:36.481914Z"
    },
    "id": "T9m_dW0I5_5p"
   },
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # 30 -> 16 -> 16 -> 1\n",
    "    self.dense0 = nn.Linear(30, 16)\n",
    "    torch.nn.init.uniform_(self.dense0.weight)\n",
    "    self.activation0 = nn.ReLU()\n",
    "    self.dense1 = nn.Linear(16, 16)\n",
    "    torch.nn.init.uniform_(self.dense1.weight)\n",
    "    self.activation1 = nn.ReLU()\n",
    "    self.dense2 = nn.Linear(16, 1)\n",
    "    torch.nn.init.uniform_(self.dense2.weight)\n",
    "    self.output = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.dense0(X)\n",
    "    X = self.activation0(X)\n",
    "    X = self.dense1(X)\n",
    "    X = self.activation1(X)\n",
    "    X = self.dense2(X)\n",
    "    X = self.output(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kstsTBpKj3yO"
   },
   "source": [
    "## Etapa 4: Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:37.629028Z",
     "start_time": "2020-10-07T23:41:37.621775Z"
    },
    "id": "YpnhSH7z8mCH"
   },
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
    "                                                  criterion=torch.nn.BCELoss,\n",
    "                                                  optimizer=torch.optim.Adam,\n",
    "                                                  lr=0.001,\n",
    "                                                  optimizer__weight_decay=0.0001,\n",
    "                                                  max_epochs=100,\n",
    "                                                  batch_size=10,\n",
    "                                                  train_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5snMQs0lGRb"
   },
   "source": [
    "## Etapa 5: Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T23:41:54.399918Z",
     "start_time": "2020-10-07T23:41:38.728412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hJqI_cFjJNk4",
    "outputId": "88d10c8d-7ab1-410e-d5de-1afb02c540d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.2537\u001b[0m  0.1480\n",
      "      2       10.2537  0.1341\n",
      "      3       10.2537  0.1360\n",
      "      4       10.2537  0.1375\n",
      "      5       10.2537  0.1390\n",
      "      6       10.2537  0.1383\n",
      "      7       10.2537  0.1398\n",
      "      8       10.2537  0.1343\n",
      "      9       10.2537  0.1459\n",
      "     10       10.2537  0.1397\n",
      "     11       10.2537  0.1405\n",
      "     12       10.2537  0.1420\n",
      "     13       10.2537  0.1354\n",
      "     14       10.2537  0.1410\n",
      "     15       10.2537  0.1287\n",
      "     16       10.2537  0.1363\n",
      "     17       10.2537  0.1490\n",
      "     18       10.2537  0.1402\n",
      "     19       10.2537  0.1382\n",
      "     20       10.2537  0.1569\n",
      "     21       10.2537  0.1478\n",
      "     22       10.2537  0.1450\n",
      "     23       10.2537  0.1342\n",
      "     24       10.2537  0.1375\n",
      "     25        \u001b[36m4.4018\u001b[0m  0.1391\n",
      "     26        \u001b[36m0.5466\u001b[0m  0.1391\n",
      "     27        \u001b[36m0.5314\u001b[0m  0.1454\n",
      "     28        \u001b[36m0.4981\u001b[0m  0.1439\n",
      "     29        \u001b[36m0.4820\u001b[0m  0.1430\n",
      "     30        \u001b[36m0.4769\u001b[0m  0.1451\n",
      "     31        \u001b[36m0.4528\u001b[0m  0.1413\n",
      "     32        \u001b[36m0.4508\u001b[0m  0.1359\n",
      "     33        \u001b[36m0.4186\u001b[0m  0.1377\n",
      "     34        \u001b[36m0.4092\u001b[0m  0.1354\n",
      "     35        \u001b[36m0.3940\u001b[0m  0.1346\n",
      "     36        0.3990  0.1291\n",
      "     37        \u001b[36m0.3844\u001b[0m  0.1457\n",
      "     38        0.3890  0.1541\n",
      "     39        \u001b[36m0.3808\u001b[0m  0.1441\n",
      "     40        \u001b[36m0.3770\u001b[0m  0.1511\n",
      "     41        \u001b[36m0.3757\u001b[0m  0.1375\n",
      "     42        \u001b[36m0.3633\u001b[0m  0.1459\n",
      "     43        0.3776  0.1410\n",
      "     44        \u001b[36m0.3622\u001b[0m  0.1381\n",
      "     45        \u001b[36m0.3586\u001b[0m  0.1531\n",
      "     46        \u001b[36m0.3565\u001b[0m  0.1427\n",
      "     47        \u001b[36m0.3561\u001b[0m  0.1392\n",
      "     48        \u001b[36m0.3448\u001b[0m  0.1411\n",
      "     49        \u001b[36m0.3376\u001b[0m  0.1301\n",
      "     50        \u001b[36m0.3326\u001b[0m  0.1427\n",
      "     51        \u001b[36m0.3287\u001b[0m  0.1424\n",
      "     52        \u001b[36m0.3281\u001b[0m  0.1383\n",
      "     53        \u001b[36m0.3185\u001b[0m  0.1358\n",
      "     54        0.3192  0.1285\n",
      "     55        \u001b[36m0.3134\u001b[0m  0.1329\n",
      "     56        \u001b[36m0.3122\u001b[0m  0.1383\n",
      "     57        \u001b[36m0.3085\u001b[0m  0.1345\n",
      "     58        0.3091  0.1388\n",
      "     59        \u001b[36m0.3023\u001b[0m  0.1335\n",
      "     60        0.3064  0.1384\n",
      "     61        \u001b[36m0.2848\u001b[0m  0.1340\n",
      "     62        \u001b[36m0.2841\u001b[0m  0.1428\n",
      "     63        \u001b[36m0.2736\u001b[0m  0.1356\n",
      "     64        \u001b[36m0.2597\u001b[0m  0.1401\n",
      "     65        \u001b[36m0.2542\u001b[0m  0.1355\n",
      "     66        \u001b[36m0.2456\u001b[0m  0.1442\n",
      "     67        0.2523  0.1395\n",
      "     68        \u001b[36m0.2404\u001b[0m  0.1328\n",
      "     69        0.2422  0.1453\n",
      "     70        \u001b[36m0.2398\u001b[0m  0.1347\n",
      "     71        \u001b[36m0.2326\u001b[0m  0.1342\n",
      "     72        \u001b[36m0.2284\u001b[0m  0.1327\n",
      "     73        \u001b[36m0.2233\u001b[0m  0.1373\n",
      "     74        \u001b[36m0.2222\u001b[0m  0.1277\n",
      "     75        0.2230  0.1350\n",
      "     76        \u001b[36m0.2194\u001b[0m  0.1394\n",
      "     77        0.2233  0.1371\n",
      "     78        \u001b[36m0.2166\u001b[0m  0.1305\n",
      "     79        0.2256  0.1344\n",
      "     80        \u001b[36m0.2163\u001b[0m  0.1336\n",
      "     81        \u001b[36m0.2071\u001b[0m  0.1320\n",
      "     82        \u001b[36m0.1942\u001b[0m  0.1371\n",
      "     83        \u001b[36m0.1896\u001b[0m  0.1423\n",
      "     84        \u001b[36m0.1806\u001b[0m  0.1351\n",
      "     85        0.1909  0.1366\n",
      "     86        0.1957  0.1304\n",
      "     87        0.1828  0.1304\n",
      "     88        \u001b[36m0.1764\u001b[0m  0.1313\n",
      "     89        0.1782  0.1384\n",
      "     90        0.1826  0.1407\n",
      "     91        \u001b[36m0.1672\u001b[0m  0.1364\n",
      "     92        \u001b[36m0.1602\u001b[0m  0.1280\n",
      "     93        0.1607  0.1340\n",
      "     94        \u001b[36m0.1569\u001b[0m  0.1429\n",
      "     95        0.1586  0.1439\n",
      "     96        0.1585  0.1396\n",
      "     97        0.1575  0.1435\n",
      "     98        0.1621  0.1461\n",
      "     99        0.1595  0.1333\n",
      "    100        0.1646  0.1452\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-f7e798ea2cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassificador_sklearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevisores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[1;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 242\u001b[0;31m     scores = parallel(\n\u001b[0m\u001b[1;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scorers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 score = scorer._score(cached_call, estimator,\n\u001b[0m\u001b[1;32m     88\u001b[0m                                       *args, **kwargs)\n\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/dev/anaconda3/lib/python3.8/site-packages/skorch/classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \"\"\"\n\u001b[1;32m    357\u001b[0m         \u001b[0my_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_proba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn, previsores, classe, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "r4o5859fKM1q",
    "outputId": "68657077-57ff-4c66-d6be-01fc1bf6e756"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "t2tLKmkuKQsM",
    "outputId": "8d9b172c-594f-4a45-8cf4-4afb6477fe50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87719298, 0.8245614 , 0.87719298, 0.63157895, 0.85964912,\n",
       "       0.92982456, 0.84210526, 0.92982456, 0.63157895, 0.625     ])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DZpQL5q0KZ-Q",
    "outputId": "c24c6421-c02b-48a0-f5fd-b89eb5a92210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8028508771929825"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9ZqSmfaCKjb0",
    "outputId": "0fd83a9f-2966-4b94-e029-a18a17d28b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11782848003734125"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desvio = resultados.std()\n",
    "desvio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMDGyU0RltXV"
   },
   "source": [
    "## Etapa 6: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU8LnHmFLyFV"
   },
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # 30 -> 16 -> 16 -> 1\n",
    "    self.dense0 = nn.Linear(30, 16)\n",
    "    torch.nn.init.uniform_(self.dense0.weight)\n",
    "    self.activation0 = nn.ReLU()\n",
    "    self.dropout0 = nn.Dropout(0.2)\n",
    "    self.dense1 = nn.Linear(16, 16)\n",
    "    torch.nn.init.uniform_(self.dense1.weight)\n",
    "    self.activation1 = nn.ReLU()\n",
    "    self.dropout1 = nn.Dropout(0.2)\n",
    "    self.dense2 = nn.Linear(16, 1)\n",
    "    torch.nn.init.uniform_(self.dense2.weight)\n",
    "    self.output = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = self.dense0(X)\n",
    "    X = self.activation0(X)\n",
    "    X = self.dropout0(X)\n",
    "    X = self.dense1(X)\n",
    "    X = self.activation1(X)\n",
    "    X = self.dropout1(X)\n",
    "    X = self.dense2(X)\n",
    "    X = self.output(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ksRGpRyMZwi"
   },
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module=classificador_torch,\n",
    "                                                  criterion=torch.nn.BCELoss,\n",
    "                                                  optimizer=torch.optim.Adam,\n",
    "                                                  lr=0.001,\n",
    "                                                  optimizer__weight_decay=0.0001,\n",
    "                                                  max_epochs=100,\n",
    "                                                  batch_size=10,\n",
    "                                                  train_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eKOttNqOMflW",
    "outputId": "6586beaf-4e70-4761-f221-da1135f43160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.2537\u001b[0m  0.1199\n",
      "      2       10.2537  0.0936\n",
      "      3       10.2537  0.0832\n",
      "      4       10.2537  0.0838\n",
      "      5       10.2537  0.0836\n",
      "      6       10.2537  0.0847\n",
      "      7       10.2537  0.0838\n",
      "      8       10.2537  0.0838\n",
      "      9       10.2537  0.0874\n",
      "     10       10.2537  0.0948\n",
      "     11       10.2537  0.0843\n",
      "     12       10.2537  0.0843\n",
      "     13       10.2537  0.0845\n",
      "     14       10.2537  0.0848\n",
      "     15       10.2537  0.0824\n",
      "     16       10.2537  0.0830\n",
      "     17       10.2537  0.0844\n",
      "     18       10.2537  0.0867\n",
      "     19       10.2537  0.0895\n",
      "     20       10.2537  0.0961\n",
      "     21       10.2537  0.0886\n",
      "     22       10.2537  0.0908\n",
      "     23       10.2537  0.0916\n",
      "     24       10.2537  0.0912\n",
      "     25       10.2537  0.0891\n",
      "     26       10.2537  0.0893\n",
      "     27       10.2537  0.0895\n",
      "     28       10.2537  0.0946\n",
      "     29       10.2537  0.0921\n",
      "     30        \u001b[36m7.1957\u001b[0m  0.0914\n",
      "     31        \u001b[36m0.6554\u001b[0m  0.0936\n",
      "     32        \u001b[36m0.6317\u001b[0m  0.0932\n",
      "     33        0.6349  0.0868\n",
      "     34        \u001b[36m0.6266\u001b[0m  0.0869\n",
      "     35        0.6295  0.0853\n",
      "     36        \u001b[36m0.5994\u001b[0m  0.0927\n",
      "     37        \u001b[36m0.5973\u001b[0m  0.0944\n",
      "     38        \u001b[36m0.5925\u001b[0m  0.0913\n",
      "     39        \u001b[36m0.5864\u001b[0m  0.0898\n",
      "     40        \u001b[36m0.5760\u001b[0m  0.0883\n",
      "     41        \u001b[36m0.5509\u001b[0m  0.0931\n",
      "     42        \u001b[36m0.5360\u001b[0m  0.0971\n",
      "     43        \u001b[36m0.5245\u001b[0m  0.0897\n",
      "     44        \u001b[36m0.4936\u001b[0m  0.0878\n",
      "     45        \u001b[36m0.4625\u001b[0m  0.0879\n",
      "     46        \u001b[36m0.4607\u001b[0m  0.0880\n",
      "     47        0.4917  0.0889\n",
      "     48        0.4607  0.0915\n",
      "     49        \u001b[36m0.4237\u001b[0m  0.0902\n",
      "     50        \u001b[36m0.4123\u001b[0m  0.0892\n",
      "     51        0.4341  0.0896\n",
      "     52        \u001b[36m0.3903\u001b[0m  0.0889\n",
      "     53        0.3946  0.0923\n",
      "     54        \u001b[36m0.3723\u001b[0m  0.0866\n",
      "     55        0.3738  0.0893\n",
      "     56        \u001b[36m0.3705\u001b[0m  0.0894\n",
      "     57        0.3739  0.0868\n",
      "     58        \u001b[36m0.3530\u001b[0m  0.0882\n",
      "     59        0.3713  0.0859\n",
      "     60        \u001b[36m0.3260\u001b[0m  0.0942\n",
      "     61        0.3592  0.0882\n",
      "     62        \u001b[36m0.3192\u001b[0m  0.0823\n",
      "     63        \u001b[36m0.3101\u001b[0m  0.0851\n",
      "     64        0.3389  0.0817\n",
      "     65        \u001b[36m0.2961\u001b[0m  0.0903\n",
      "     66        0.3299  0.0908\n",
      "     67        \u001b[36m0.2932\u001b[0m  0.0894\n",
      "     68        0.3302  0.0899\n",
      "     69        \u001b[36m0.2889\u001b[0m  0.0934\n",
      "     70        0.2963  0.0948\n",
      "     71        \u001b[36m0.2748\u001b[0m  0.0883\n",
      "     72        0.3113  0.0865\n",
      "     73        \u001b[36m0.2741\u001b[0m  0.0872\n",
      "     74        0.2808  0.0884\n",
      "     75        \u001b[36m0.2681\u001b[0m  0.0865\n",
      "     76        0.2865  0.0878\n",
      "     77        0.2687  0.0959\n",
      "     78        0.2941  0.0887\n",
      "     79        0.2882  0.0916\n",
      "     80        0.2784  0.0901\n",
      "     81        \u001b[36m0.2649\u001b[0m  0.0912\n",
      "     82        \u001b[36m0.2552\u001b[0m  0.0892\n",
      "     83        0.2710  0.0890\n",
      "     84        \u001b[36m0.2501\u001b[0m  0.0910\n",
      "     85        \u001b[36m0.2501\u001b[0m  0.0897\n",
      "     86        0.2799  0.0901\n",
      "     87        0.2517  0.0898\n",
      "     88        0.2516  0.0963\n",
      "     89        0.2783  0.0936\n",
      "     90        \u001b[36m0.2372\u001b[0m  0.0994\n",
      "     91        \u001b[36m0.2302\u001b[0m  0.0894\n",
      "     92        0.2815  0.0909\n",
      "     93        0.2489  0.0868\n",
      "     94        0.2734  0.0853\n",
      "     95        0.2392  0.0960\n",
      "     96        \u001b[36m0.2249\u001b[0m  0.0900\n",
      "     97        0.2710  0.0932\n",
      "     98        0.2830  0.1048\n",
      "     99        0.2485  0.0880\n",
      "    100        0.2460  0.0889\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.2537\u001b[0m  0.0880\n",
      "      2       10.2537  0.0873\n",
      "      3       10.2537  0.0848\n",
      "      4       10.2537  0.0881\n",
      "      5       10.2537  0.0858\n",
      "      6       10.2537  0.0914\n",
      "      7       10.2537  0.0889\n",
      "      8       10.2537  0.0870\n",
      "      9       10.2537  0.0909\n",
      "     10       10.2537  0.0875\n",
      "     11       10.2537  0.0926\n",
      "     12       10.2537  0.0952\n",
      "     13       10.2537  0.0906\n",
      "     14       10.2537  0.0892\n",
      "     15       10.2537  0.0889\n",
      "     16       10.2537  0.0966\n",
      "     17       10.2537  0.0952\n",
      "     18       10.2537  0.0896\n",
      "     19       10.2537  0.0885\n",
      "     20       \u001b[36m10.1910\u001b[0m  0.0883\n",
      "     21        \u001b[36m7.1824\u001b[0m  0.0827\n",
      "     22        \u001b[36m1.7189\u001b[0m  0.0909\n",
      "     23        \u001b[36m0.5777\u001b[0m  0.0888\n",
      "     24        0.5925  0.0884\n",
      "     25        \u001b[36m0.5733\u001b[0m  0.0827\n",
      "     26        \u001b[36m0.5079\u001b[0m  0.0915\n",
      "     27        0.5127  0.0918\n",
      "     28        0.5723  0.0921\n",
      "     29        \u001b[36m0.4981\u001b[0m  0.0885\n",
      "     30        0.5127  0.0871\n",
      "     31        0.5116  0.1008\n",
      "     32        \u001b[36m0.4860\u001b[0m  0.0903\n",
      "     33        0.4911  0.0861\n",
      "     34        \u001b[36m0.4708\u001b[0m  0.0907\n",
      "     35        \u001b[36m0.4411\u001b[0m  0.0855\n",
      "     36        \u001b[36m0.4204\u001b[0m  0.0891\n",
      "     37        \u001b[36m0.4184\u001b[0m  0.0889\n",
      "     38        \u001b[36m0.3906\u001b[0m  0.0937\n",
      "     39        0.4099  0.0853\n",
      "     40        0.3926  0.0869\n",
      "     41        \u001b[36m0.3810\u001b[0m  0.0865\n",
      "     42        \u001b[36m0.3476\u001b[0m  0.0875\n",
      "     43        0.3566  0.0924\n",
      "     44        0.3493  0.0866\n",
      "     45        \u001b[36m0.3303\u001b[0m  0.0897\n",
      "     46        0.3532  0.0881\n",
      "     47        \u001b[36m0.3271\u001b[0m  0.0867\n",
      "     48        \u001b[36m0.2951\u001b[0m  0.0999\n",
      "     49        0.2993  0.0889\n",
      "     50        0.2969  0.0825\n",
      "     51        0.3516  0.0901\n",
      "     52        0.2952  0.0932\n",
      "     53        \u001b[36m0.2912\u001b[0m  0.0915\n",
      "     54        0.3010  0.0919\n",
      "     55        \u001b[36m0.2765\u001b[0m  0.0918\n",
      "     56        \u001b[36m0.2730\u001b[0m  0.0963\n",
      "     57        0.2866  0.0893\n",
      "     58        0.2932  0.0889\n",
      "     59        0.2843  0.0872\n",
      "     60        \u001b[36m0.2444\u001b[0m  0.0904\n",
      "     61        \u001b[36m0.2422\u001b[0m  0.0968\n",
      "     62        \u001b[36m0.2220\u001b[0m  0.0902\n",
      "     63        0.2540  0.0890\n",
      "     64        0.2369  0.0862\n",
      "     65        0.2237  0.0892\n",
      "     66        0.2336  0.0885\n",
      "     67        0.2293  0.0963\n",
      "     68        0.2361  0.0943\n",
      "     69        0.2430  0.0908\n",
      "     70        0.2324  0.0899\n",
      "     71        0.2493  0.0917\n",
      "     72        0.2263  0.0905\n",
      "     73        0.2274  0.0931\n",
      "     74        0.2429  0.0939\n",
      "     75        0.2420  0.0923\n",
      "     76        \u001b[36m0.2175\u001b[0m  0.0981\n",
      "     77        \u001b[36m0.2146\u001b[0m  0.0915\n",
      "     78        0.2218  0.0913\n",
      "     79        0.2289  0.0909\n",
      "     80        \u001b[36m0.2106\u001b[0m  0.1054\n",
      "     81        0.2327  0.0919\n",
      "     82        \u001b[36m0.2084\u001b[0m  0.0915\n",
      "     83        0.2152  0.1048\n",
      "     84        \u001b[36m0.1897\u001b[0m  0.0919\n",
      "     85        \u001b[36m0.1796\u001b[0m  0.1017\n",
      "     86        0.1873  0.0889\n",
      "     87        0.1942  0.0899\n",
      "     88        \u001b[36m0.1745\u001b[0m  0.0932\n",
      "     89        0.2086  0.0900\n",
      "     90        0.1816  0.0904\n",
      "     91        0.1966  0.0930\n",
      "     92        0.1942  0.0879\n",
      "     93        0.1848  0.0939\n",
      "     94        0.1902  0.0882\n",
      "     95        0.2086  0.0839\n",
      "     96        0.1971  0.1076\n",
      "     97        0.1936  0.1009\n",
      "     98        0.1887  0.0895\n",
      "     99        0.2020  0.0918\n",
      "    100        0.1923  0.0894\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0926\n",
      "      2       10.3077  0.0960\n",
      "      3       10.3077  0.0860\n",
      "      4       10.3077  0.0877\n",
      "      5       10.3077  0.0869\n",
      "      6       10.3077  0.0897\n",
      "      7       10.3077  0.0893\n",
      "      8       10.3077  0.0878\n",
      "      9       10.3077  0.0873\n",
      "     10       10.3077  0.0987\n",
      "     11       10.3077  0.0883\n",
      "     12       10.3077  0.0891\n",
      "     13       10.3077  0.0947\n",
      "     14       10.3077  0.0852\n",
      "     15       10.3077  0.0898\n",
      "     16       10.3077  0.0907\n",
      "     17       10.3077  0.0974\n",
      "     18       10.3077  0.0937\n",
      "     19       \u001b[36m10.2899\u001b[0m  0.0866\n",
      "     20       \u001b[36m10.2796\u001b[0m  0.0862\n",
      "     21        \u001b[36m9.8466\u001b[0m  0.0869\n",
      "     22        \u001b[36m1.7717\u001b[0m  0.0865\n",
      "     23        \u001b[36m0.7134\u001b[0m  0.0922\n",
      "     24        \u001b[36m0.6103\u001b[0m  0.0828\n",
      "     25        \u001b[36m0.5756\u001b[0m  0.0832\n",
      "     26        0.5787  0.0876\n",
      "     27        \u001b[36m0.5718\u001b[0m  0.0891\n",
      "     28        \u001b[36m0.5323\u001b[0m  0.0860\n",
      "     29        0.5543  0.0888\n",
      "     30        0.5615  0.0949\n",
      "     31        0.5331  0.0894\n",
      "     32        \u001b[36m0.5272\u001b[0m  0.0869\n",
      "     33        0.5344  0.0859\n",
      "     34        0.5387  0.0903\n",
      "     35        \u001b[36m0.5248\u001b[0m  0.0891\n",
      "     36        0.5381  0.0923\n",
      "     37        0.5264  0.0940\n",
      "     38        \u001b[36m0.5144\u001b[0m  0.0889\n",
      "     39        0.5450  0.0870\n",
      "     40        \u001b[36m0.5037\u001b[0m  0.0914\n",
      "     41        0.5104  0.0898\n",
      "     42        \u001b[36m0.4693\u001b[0m  0.0913\n",
      "     43        0.4755  0.0935\n",
      "     44        \u001b[36m0.4574\u001b[0m  0.0887\n",
      "     45        0.4884  0.0867\n",
      "     46        \u001b[36m0.4462\u001b[0m  0.0873\n",
      "     47        \u001b[36m0.3979\u001b[0m  0.0916\n",
      "     48        0.4001  0.0888\n",
      "     49        0.4187  0.0889\n",
      "     50        \u001b[36m0.3770\u001b[0m  0.1004\n",
      "     51        \u001b[36m0.3603\u001b[0m  0.0862\n",
      "     52        0.3900  0.0864\n",
      "     53        0.3905  0.0897\n",
      "     54        0.4261  0.0884\n",
      "     55        \u001b[36m0.3504\u001b[0m  0.0905\n",
      "     56        0.3576  0.0885\n",
      "     57        \u001b[36m0.3386\u001b[0m  0.0875\n",
      "     58        0.3400  0.1035\n",
      "     59        0.3638  0.0875\n",
      "     60        0.3420  0.0889\n",
      "     61        0.3451  0.0874\n",
      "     62        \u001b[36m0.3313\u001b[0m  0.0890\n",
      "     63        \u001b[36m0.3099\u001b[0m  0.0972\n",
      "     64        0.3175  0.0882\n",
      "     65        0.3102  0.0897\n",
      "     66        0.3458  0.0882\n",
      "     67        0.3342  0.0924\n",
      "     68        0.3321  0.0882\n",
      "     69        0.3118  0.0865\n",
      "     70        \u001b[36m0.2983\u001b[0m  0.0872\n",
      "     71        0.3417  0.0956\n",
      "     72        \u001b[36m0.2970\u001b[0m  0.0988\n",
      "     73        0.3204  0.0845\n",
      "     74        0.3060  0.0933\n",
      "     75        0.3082  0.0945\n",
      "     76        0.3038  0.0901\n",
      "     77        \u001b[36m0.2801\u001b[0m  0.0904\n",
      "     78        0.2868  0.0907\n",
      "     79        \u001b[36m0.2769\u001b[0m  0.0926\n",
      "     80        0.2993  0.0878\n",
      "     81        0.2966  0.0940\n",
      "     82        0.3173  0.0915\n",
      "     83        0.3159  0.1035\n",
      "     84        0.3402  0.0878\n",
      "     85        0.2973  0.0883\n",
      "     86        \u001b[36m0.2763\u001b[0m  0.0971\n",
      "     87        0.2995  0.0939\n",
      "     88        \u001b[36m0.2761\u001b[0m  0.0945\n",
      "     89        \u001b[36m0.2659\u001b[0m  0.0894\n",
      "     90        0.2790  0.0954\n",
      "     91        \u001b[36m0.2654\u001b[0m  0.0896\n",
      "     92        0.2755  0.0937\n",
      "     93        \u001b[36m0.2571\u001b[0m  0.0906\n",
      "     94        \u001b[36m0.2570\u001b[0m  0.0921\n",
      "     95        0.2607  0.0896\n",
      "     96        0.2788  0.0902\n",
      "     97        0.2660  0.0877\n",
      "     98        0.2652  0.0932\n",
      "     99        \u001b[36m0.2505\u001b[0m  0.0935\n",
      "    100        0.2741  0.0846\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0817\n",
      "      2       10.3077  0.0812\n",
      "      3       10.3077  0.0870\n",
      "      4       10.3077  0.1020\n",
      "      5       10.3077  0.0918\n",
      "      6       10.3077  0.1001\n",
      "      7       10.3077  0.0928\n",
      "      8       10.3077  0.0881\n",
      "      9       10.3077  0.0924\n",
      "     10       10.3077  0.0891\n",
      "     11       10.3077  0.0888\n",
      "     12       10.3077  0.0870\n",
      "     13       10.3077  0.0877\n",
      "     14       10.3077  0.0863\n",
      "     15       10.3077  0.0891\n",
      "     16       10.3077  0.0886\n",
      "     17       10.3077  0.0890\n",
      "     18       10.3077  0.0899\n",
      "     19       10.3077  0.0919\n",
      "     20       10.3077  0.0966\n",
      "     21       10.3077  0.0894\n",
      "     22       10.3077  0.0912\n",
      "     23       \u001b[36m10.2835\u001b[0m  0.0883\n",
      "     24        \u001b[36m3.4025\u001b[0m  0.0886\n",
      "     25        \u001b[36m0.6338\u001b[0m  0.0917\n",
      "     26        \u001b[36m0.5970\u001b[0m  0.0882\n",
      "     27        \u001b[36m0.5830\u001b[0m  0.0890\n",
      "     28        \u001b[36m0.5545\u001b[0m  0.0884\n",
      "     29        0.5612  0.0905\n",
      "     30        \u001b[36m0.5524\u001b[0m  0.0950\n",
      "     31        \u001b[36m0.5347\u001b[0m  0.0869\n",
      "     32        \u001b[36m0.5149\u001b[0m  0.0847\n",
      "     33        0.5200  0.0896\n",
      "     34        \u001b[36m0.4982\u001b[0m  0.0894\n",
      "     35        \u001b[36m0.4853\u001b[0m  0.0897\n",
      "     36        0.4999  0.0892\n",
      "     37        \u001b[36m0.4768\u001b[0m  0.0875\n",
      "     38        \u001b[36m0.4741\u001b[0m  0.0913\n",
      "     39        0.4911  0.0927\n",
      "     40        \u001b[36m0.4678\u001b[0m  0.0941\n",
      "     41        \u001b[36m0.4649\u001b[0m  0.0869\n",
      "     42        \u001b[36m0.4494\u001b[0m  0.0868\n",
      "     43        \u001b[36m0.4389\u001b[0m  0.0844\n",
      "     44        \u001b[36m0.4100\u001b[0m  0.0892\n",
      "     45        0.4139  0.0913\n",
      "     46        0.4178  0.0908\n",
      "     47        0.4253  0.0874\n",
      "     48        \u001b[36m0.4039\u001b[0m  0.0856\n",
      "     49        0.4156  0.0879\n",
      "     50        \u001b[36m0.3702\u001b[0m  0.0890\n",
      "     51        0.3939  0.0916\n",
      "     52        \u001b[36m0.3612\u001b[0m  0.0943\n",
      "     53        \u001b[36m0.3597\u001b[0m  0.0851\n",
      "     54        0.3683  0.0875\n",
      "     55        \u001b[36m0.3500\u001b[0m  0.0883\n",
      "     56        0.3633  0.0889\n",
      "     57        0.3511  0.0873\n",
      "     58        0.3684  0.0860\n",
      "     59        0.3564  0.0938\n",
      "     60        \u001b[36m0.3341\u001b[0m  0.0915\n",
      "     61        0.3427  0.0914\n",
      "     62        0.3370  0.0906\n",
      "     63        \u001b[36m0.3259\u001b[0m  0.0902\n",
      "     64        \u001b[36m0.3107\u001b[0m  0.0933\n",
      "     65        \u001b[36m0.2868\u001b[0m  0.0901\n",
      "     66        0.2995  0.0889\n",
      "     67        0.3209  0.0904\n",
      "     68        0.3056  0.0889\n",
      "     69        0.2962  0.0946\n",
      "     70        0.3049  0.0905\n",
      "     71        \u001b[36m0.2802\u001b[0m  0.0879\n",
      "     72        0.2927  0.0853\n",
      "     73        0.3024  0.0893\n",
      "     74        0.2945  0.0875\n",
      "     75        0.2880  0.0878\n",
      "     76        0.2927  0.0887\n",
      "     77        0.2847  0.0873\n",
      "     78        \u001b[36m0.2695\u001b[0m  0.0885\n",
      "     79        0.2916  0.0983\n",
      "     80        0.2923  0.0860\n",
      "     81        0.2900  0.0928\n",
      "     82        0.2793  0.0930\n",
      "     83        0.2751  0.0902\n",
      "     84        \u001b[36m0.2623\u001b[0m  0.0886\n",
      "     85        0.2777  0.0864\n",
      "     86        0.2788  0.0911\n",
      "     87        0.3125  0.0924\n",
      "     88        0.2974  0.0965\n",
      "     89        \u001b[36m0.2600\u001b[0m  0.0886\n",
      "     90        0.3000  0.0830\n",
      "     91        0.2708  0.0939\n",
      "     92        0.2684  0.0910\n",
      "     93        0.2671  0.0976\n",
      "     94        0.2832  0.0957\n",
      "     95        \u001b[36m0.2471\u001b[0m  0.0917\n",
      "     96        0.2719  0.0963\n",
      "     97        0.2549  0.0835\n",
      "     98        0.2573  0.0931\n",
      "     99        0.2651  0.0881\n",
      "    100        0.2530  0.0905\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0872\n",
      "      2       10.3077  0.0894\n",
      "      3       10.3077  0.0877\n",
      "      4       10.3077  0.0904\n",
      "      5       10.3077  0.0958\n",
      "      6       10.3077  0.1104\n",
      "      7       10.3077  0.0957\n",
      "      8       10.3077  0.0930\n",
      "      9       10.3077  0.0882\n",
      "     10       10.3077  0.0893\n",
      "     11       10.3077  0.0911\n",
      "     12       10.3077  0.0911\n",
      "     13       10.3077  0.0890\n",
      "     14       10.3077  0.0897\n",
      "     15       10.3077  0.0896\n",
      "     16       10.3077  0.0920\n",
      "     17       10.3077  0.0886\n",
      "     18       10.3077  0.0882\n",
      "     19       10.3077  0.0877\n",
      "     20       10.3077  0.0936\n",
      "     21       10.3077  0.0963\n",
      "     22       10.3077  0.0987\n",
      "     23       10.3077  0.0891\n",
      "     24        \u001b[36m5.9126\u001b[0m  0.0878\n",
      "     25        \u001b[36m0.5718\u001b[0m  0.0930\n",
      "     26        \u001b[36m0.5526\u001b[0m  0.0888\n",
      "     27        \u001b[36m0.5071\u001b[0m  0.0935\n",
      "     28        \u001b[36m0.5045\u001b[0m  0.0855\n",
      "     29        \u001b[36m0.4996\u001b[0m  0.0866\n",
      "     30        \u001b[36m0.4832\u001b[0m  0.0869\n",
      "     31        \u001b[36m0.4580\u001b[0m  0.0875\n",
      "     32        \u001b[36m0.4478\u001b[0m  0.0901\n",
      "     33        \u001b[36m0.4403\u001b[0m  0.0898\n",
      "     34        \u001b[36m0.4310\u001b[0m  0.0936\n",
      "     35        \u001b[36m0.4147\u001b[0m  0.0878\n",
      "     36        \u001b[36m0.3842\u001b[0m  0.0855\n",
      "     37        0.4178  0.0892\n",
      "     38        0.3881  0.0890\n",
      "     39        \u001b[36m0.3779\u001b[0m  0.0883\n",
      "     40        \u001b[36m0.3759\u001b[0m  0.0910\n",
      "     41        \u001b[36m0.3558\u001b[0m  0.0910\n",
      "     42        \u001b[36m0.3416\u001b[0m  0.0922\n",
      "     43        0.3688  0.0880\n",
      "     44        0.3545  0.0890\n",
      "     45        \u001b[36m0.3366\u001b[0m  0.0888\n",
      "     46        \u001b[36m0.3351\u001b[0m  0.0908\n",
      "     47        \u001b[36m0.3239\u001b[0m  0.0892\n",
      "     48        \u001b[36m0.3166\u001b[0m  0.0905\n",
      "     49        0.3465  0.0911\n",
      "     50        0.3268  0.0870\n",
      "     51        \u001b[36m0.3048\u001b[0m  0.0934\n",
      "     52        0.3159  0.0911\n",
      "     53        \u001b[36m0.2924\u001b[0m  0.0884\n",
      "     54        0.3068  0.0901\n",
      "     55        0.2973  0.0893\n",
      "     56        \u001b[36m0.2620\u001b[0m  0.0879\n",
      "     57        \u001b[36m0.2585\u001b[0m  0.0989\n",
      "     58        0.3061  0.1031\n",
      "     59        0.2646  0.0864\n",
      "     60        \u001b[36m0.2537\u001b[0m  0.0805\n",
      "     61        0.3104  0.0889\n",
      "     62        \u001b[36m0.2509\u001b[0m  0.0892\n",
      "     63        0.2989  0.0893\n",
      "     64        0.2585  0.0902\n",
      "     65        0.2751  0.0882\n",
      "     66        0.2677  0.0946\n",
      "     67        0.3365  0.0810\n",
      "     68        0.2695  0.0865\n",
      "     69        0.2740  0.0871\n",
      "     70        0.2600  0.0930\n",
      "     71        0.2612  0.0900\n",
      "     72        0.2869  0.0884\n",
      "     73        \u001b[36m0.2225\u001b[0m  0.0910\n",
      "     74        0.2615  0.0901\n",
      "     75        0.2636  0.0901\n",
      "     76        0.2476  0.0929\n",
      "     77        0.2545  0.0871\n",
      "     78        0.2714  0.0871\n",
      "     79        0.2390  0.0934\n",
      "     80        0.2597  0.0888\n",
      "     81        0.2412  0.0946\n",
      "     82        0.2508  0.0895\n",
      "     83        0.2272  0.0909\n",
      "     84        0.2333  0.0891\n",
      "     85        0.2281  0.0875\n",
      "     86        0.2308  0.0874\n",
      "     87        0.2468  0.0927\n",
      "     88        0.2241  0.0915\n",
      "     89        \u001b[36m0.2147\u001b[0m  0.0866\n",
      "     90        0.2240  0.0905\n",
      "     91        0.2635  0.0839\n",
      "     92        0.2633  0.0898\n",
      "     93        0.2224  0.0977\n",
      "     94        0.2253  0.0877\n",
      "     95        \u001b[36m0.2094\u001b[0m  0.0884\n",
      "     96        0.2147  0.0920\n",
      "     97        0.2302  0.0914\n",
      "     98        0.2310  0.0892\n",
      "     99        0.2500  0.0924\n",
      "    100        \u001b[36m0.2077\u001b[0m  0.0880\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0841\n",
      "      2       10.3077  0.0888\n",
      "      3       10.3077  0.0908\n",
      "      4       10.3077  0.0878\n",
      "      5       10.3077  0.0883\n",
      "      6       10.3077  0.0973\n",
      "      7       10.3077  0.0890\n",
      "      8       10.3077  0.0841\n",
      "      9       10.3077  0.0867\n",
      "     10       10.3077  0.0859\n",
      "     11       10.3077  0.0853\n",
      "     12       10.3077  0.0902\n",
      "     13       10.3077  0.0980\n",
      "     14       10.3077  0.0893\n",
      "     15       10.3077  0.0909\n",
      "     16       10.3077  0.0897\n",
      "     17       10.3077  0.0895\n",
      "     18       10.3077  0.0859\n",
      "     19       10.3077  0.0902\n",
      "     20       10.3077  0.0876\n",
      "     21       10.3077  0.0868\n",
      "     22       10.3077  0.0875\n",
      "     23       10.3077  0.0907\n",
      "     24       10.3077  0.0939\n",
      "     25       10.3077  0.0887\n",
      "     26        \u001b[36m3.9233\u001b[0m  0.0888\n",
      "     27        \u001b[36m0.6084\u001b[0m  0.0883\n",
      "     28        \u001b[36m0.5746\u001b[0m  0.0883\n",
      "     29        \u001b[36m0.5386\u001b[0m  0.0858\n",
      "     30        \u001b[36m0.5186\u001b[0m  0.0821\n",
      "     31        \u001b[36m0.4954\u001b[0m  0.0888\n",
      "     32        \u001b[36m0.4780\u001b[0m  0.0889\n",
      "     33        0.4838  0.0971\n",
      "     34        \u001b[36m0.4596\u001b[0m  0.1012\n",
      "     35        \u001b[36m0.4255\u001b[0m  0.0889\n",
      "     36        0.4439  0.0875\n",
      "     37        0.4308  0.0900\n",
      "     38        \u001b[36m0.4129\u001b[0m  0.0918\n",
      "     39        0.4175  0.1003\n",
      "     40        0.4175  0.0904\n",
      "     41        \u001b[36m0.4041\u001b[0m  0.0904\n",
      "     42        \u001b[36m0.3962\u001b[0m  0.0905\n",
      "     43        \u001b[36m0.3854\u001b[0m  0.0907\n",
      "     44        \u001b[36m0.3697\u001b[0m  0.1007\n",
      "     45        0.3738  0.0974\n",
      "     46        \u001b[36m0.3620\u001b[0m  0.0885\n",
      "     47        0.3815  0.0872\n",
      "     48        0.3638  0.0882\n",
      "     49        0.3852  0.0892\n",
      "     50        \u001b[36m0.3466\u001b[0m  0.0922\n",
      "     51        \u001b[36m0.3418\u001b[0m  0.0903\n",
      "     52        0.3708  0.0920\n",
      "     53        0.3736  0.0891\n",
      "     54        0.3555  0.0887\n",
      "     55        \u001b[36m0.3408\u001b[0m  0.0923\n",
      "     56        0.3566  0.0865\n",
      "     57        0.3757  0.0923\n",
      "     58        0.3481  0.1034\n",
      "     59        0.3479  0.0898\n",
      "     60        \u001b[36m0.3401\u001b[0m  0.0899\n",
      "     61        0.3506  0.1010\n",
      "     62        \u001b[36m0.3326\u001b[0m  0.0903\n",
      "     63        \u001b[36m0.3140\u001b[0m  0.0999\n",
      "     64        0.3336  0.0895\n",
      "     65        0.3685  0.0861\n",
      "     66        0.3297  0.0877\n",
      "     67        0.3272  0.0886\n",
      "     68        0.3174  0.0856\n",
      "     69        0.3181  0.0866\n",
      "     70        \u001b[36m0.3094\u001b[0m  0.0850\n",
      "     71        0.3359  0.0858\n",
      "     72        0.3302  0.0964\n",
      "     73        0.3145  0.0862\n",
      "     74        0.3313  0.0866\n",
      "     75        0.3351  0.0857\n",
      "     76        0.3194  0.0918\n",
      "     77        0.3171  0.0900\n",
      "     78        \u001b[36m0.3008\u001b[0m  0.0909\n",
      "     79        0.3361  0.0867\n",
      "     80        \u001b[36m0.2874\u001b[0m  0.0880\n",
      "     81        0.2895  0.0888\n",
      "     82        0.3267  0.0930\n",
      "     83        \u001b[36m0.2726\u001b[0m  0.0887\n",
      "     84        0.2830  0.0936\n",
      "     85        0.3117  0.0868\n",
      "     86        0.2840  0.0891\n",
      "     87        0.2880  0.0918\n",
      "     88        0.3129  0.0978\n",
      "     89        0.3145  0.0915\n",
      "     90        0.3162  0.0931\n",
      "     91        0.3018  0.0977\n",
      "     92        0.3039  0.0916\n",
      "     93        0.2862  0.0916\n",
      "     94        0.2981  0.0927\n",
      "     95        0.3267  0.0961\n",
      "     96        0.3376  0.1072\n",
      "     97        0.3137  0.0927\n",
      "     98        0.3537  0.0927\n",
      "     99        0.3047  0.0986\n",
      "    100        0.3054  0.0903\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0816\n",
      "      2       10.3077  0.0876\n",
      "      3       10.3077  0.0910\n",
      "      4       10.3077  0.0908\n",
      "      5       10.3077  0.0902\n",
      "      6       10.3077  0.0907\n",
      "      7       10.3077  0.0901\n",
      "      8       10.3077  0.0886\n",
      "      9       10.3077  0.0991\n",
      "     10       10.3077  0.0895\n",
      "     11       10.3077  0.0897\n",
      "     12       10.3077  0.0887\n",
      "     13       10.3077  0.0937\n",
      "     14       10.3077  0.1076\n",
      "     15       10.3077  0.0865\n",
      "     16       10.3077  0.0860\n",
      "     17       10.3077  0.0878\n",
      "     18       10.3077  0.0879\n",
      "     19       10.3077  0.0886\n",
      "     20       10.3077  0.0953\n",
      "     21       10.3077  0.0902\n",
      "     22       10.3077  0.0805\n",
      "     23       10.3077  0.0860\n",
      "     24       10.3077  0.0879\n",
      "     25       10.3077  0.0878\n",
      "     26       10.3077  0.0888\n",
      "     27       10.3077  0.0899\n",
      "     28        \u001b[36m5.1101\u001b[0m  0.0887\n",
      "     29        \u001b[36m0.6052\u001b[0m  0.0843\n",
      "     30        \u001b[36m0.6006\u001b[0m  0.0955\n",
      "     31        \u001b[36m0.5748\u001b[0m  0.0881\n",
      "     32        0.5778  0.0891\n",
      "     33        \u001b[36m0.5692\u001b[0m  0.0890\n",
      "     34        \u001b[36m0.5493\u001b[0m  0.0876\n",
      "     35        0.5691  0.1049\n",
      "     36        0.5605  0.0931\n",
      "     37        0.5622  0.0893\n",
      "     38        0.5563  0.0900\n",
      "     39        0.5585  0.0932\n",
      "     40        0.5540  0.0928\n",
      "     41        0.5539  0.0857\n",
      "     42        0.5633  0.0844\n",
      "     43        0.5698  0.0867\n",
      "     44        \u001b[36m0.5394\u001b[0m  0.0866\n",
      "     45        0.5503  0.0897\n",
      "     46        0.5528  0.0866\n",
      "     47        0.5429  0.0856\n",
      "     48        0.5563  0.0925\n",
      "     49        0.5437  0.0873\n",
      "     50        \u001b[36m0.5354\u001b[0m  0.0868\n",
      "     51        0.5519  0.0854\n",
      "     52        0.5392  0.0920\n",
      "     53        0.5731  0.0861\n",
      "     54        0.5509  0.0871\n",
      "     55        0.5383  0.0907\n",
      "     56        0.5632  0.0901\n",
      "     57        0.5370  0.0876\n",
      "     58        0.5512  0.0877\n",
      "     59        0.5468  0.0908\n",
      "     60        0.5737  0.0947\n",
      "     61        \u001b[36m0.5342\u001b[0m  0.0893\n",
      "     62        0.5429  0.0946\n",
      "     63        0.5413  0.0967\n",
      "     64        0.5811  0.0869\n",
      "     65        0.5458  0.0894\n",
      "     66        \u001b[36m0.5336\u001b[0m  0.0852\n",
      "     67        0.5360  0.0867\n",
      "     68        0.5401  0.0933\n",
      "     69        0.5383  0.0898\n",
      "     70        0.5379  0.0920\n",
      "     71        0.5461  0.0882\n",
      "     72        0.5467  0.0834\n",
      "     73        0.5351  0.0916\n",
      "     74        0.5712  0.1074\n",
      "     75        \u001b[36m0.5310\u001b[0m  0.0908\n",
      "     76        0.5457  0.0890\n",
      "     77        0.5402  0.0888\n",
      "     78        0.5439  0.0875\n",
      "     79        0.5581  0.0897\n",
      "     80        0.5358  0.0889\n",
      "     81        \u001b[36m0.5279\u001b[0m  0.0899\n",
      "     82        0.5447  0.0929\n",
      "     83        0.5369  0.0902\n",
      "     84        0.5321  0.0926\n",
      "     85        0.5418  0.0974\n",
      "     86        0.5368  0.0916\n",
      "     87        0.5366  0.0892\n",
      "     88        0.5354  0.0923\n",
      "     89        0.5343  0.0890\n",
      "     90        0.5362  0.0891\n",
      "     91        0.5411  0.0918\n",
      "     92        0.5315  0.0926\n",
      "     93        0.5554  0.0920\n",
      "     94        0.5391  0.0924\n",
      "     95        0.5289  0.0909\n",
      "     96        \u001b[36m0.5148\u001b[0m  0.1040\n",
      "     97        0.5209  0.0887\n",
      "     98        0.5152  0.0904\n",
      "     99        \u001b[36m0.5041\u001b[0m  0.0911\n",
      "    100        0.5566  0.0905\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0866\n",
      "      2       10.3077  0.0851\n",
      "      3       10.3077  0.0861\n",
      "      4       10.3077  0.0910\n",
      "      5       10.3077  0.0914\n",
      "      6       10.3077  0.0929\n",
      "      7       10.3077  0.0889\n",
      "      8       10.3077  0.0847\n",
      "      9       10.3077  0.0895\n",
      "     10       10.3077  0.0926\n",
      "     11       10.3077  0.0904\n",
      "     12       10.3077  0.0915\n",
      "     13       10.3077  0.1145\n",
      "     14       10.3077  0.0959\n",
      "     15       10.3077  0.0924\n",
      "     16       10.3077  0.0969\n",
      "     17       10.3077  0.0879\n",
      "     18       10.3077  0.0896\n",
      "     19       10.3077  0.0876\n",
      "     20       10.3077  0.0864\n",
      "     21       10.3077  0.0866\n",
      "     22       10.3077  0.0888\n",
      "     23       10.3077  0.0875\n",
      "     24       10.3077  0.0895\n",
      "     25        \u001b[36m5.6481\u001b[0m  0.0889\n",
      "     26        \u001b[36m0.6341\u001b[0m  0.0852\n",
      "     27        \u001b[36m0.6149\u001b[0m  0.0935\n",
      "     28        0.6191  0.0908\n",
      "     29        \u001b[36m0.6148\u001b[0m  0.0923\n",
      "     30        \u001b[36m0.6041\u001b[0m  0.0925\n",
      "     31        \u001b[36m0.6036\u001b[0m  0.0899\n",
      "     32        0.6044  0.0959\n",
      "     33        \u001b[36m0.5809\u001b[0m  0.0901\n",
      "     34        0.6084  0.0889\n",
      "     35        \u001b[36m0.5433\u001b[0m  0.0897\n",
      "     36        0.5610  0.0905\n",
      "     37        \u001b[36m0.5277\u001b[0m  0.0941\n",
      "     38        0.5292  0.0871\n",
      "     39        0.5303  0.0860\n",
      "     40        \u001b[36m0.5049\u001b[0m  0.0888\n",
      "     41        \u001b[36m0.4902\u001b[0m  0.0935\n",
      "     42        \u001b[36m0.4802\u001b[0m  0.0946\n",
      "     43        \u001b[36m0.4503\u001b[0m  0.0905\n",
      "     44        0.4766  0.0914\n",
      "     45        0.4604  0.1037\n",
      "     46        \u001b[36m0.4487\u001b[0m  0.0885\n",
      "     47        \u001b[36m0.4248\u001b[0m  0.0933\n",
      "     48        0.4353  0.0893\n",
      "     49        \u001b[36m0.4220\u001b[0m  0.0917\n",
      "     50        0.4362  0.0912\n",
      "     51        \u001b[36m0.3930\u001b[0m  0.0913\n",
      "     52        \u001b[36m0.3647\u001b[0m  0.0895\n",
      "     53        0.3974  0.0888\n",
      "     54        0.3786  0.0917\n",
      "     55        \u001b[36m0.3540\u001b[0m  0.0878\n",
      "     56        0.3769  0.0872\n",
      "     57        0.3800  0.0846\n",
      "     58        \u001b[36m0.3279\u001b[0m  0.0914\n",
      "     59        0.3716  0.0920\n",
      "     60        0.3445  0.0899\n",
      "     61        0.3571  0.0904\n",
      "     62        0.3348  0.0875\n",
      "     63        \u001b[36m0.3216\u001b[0m  0.0879\n",
      "     64        0.3629  0.0878\n",
      "     65        \u001b[36m0.3142\u001b[0m  0.0872\n",
      "     66        0.3297  0.0995\n",
      "     67        0.3364  0.0859\n",
      "     68        0.3320  0.0871\n",
      "     69        0.3447  0.0907\n",
      "     70        0.3297  0.0970\n",
      "     71        0.3280  0.0875\n",
      "     72        \u001b[36m0.2998\u001b[0m  0.0898\n",
      "     73        0.3191  0.0922\n",
      "     74        \u001b[36m0.2927\u001b[0m  0.0904\n",
      "     75        0.2978  0.0971\n",
      "     76        0.3215  0.0898\n",
      "     77        0.3216  0.0864\n",
      "     78        \u001b[36m0.2907\u001b[0m  0.0953\n",
      "     79        \u001b[36m0.2820\u001b[0m  0.0928\n",
      "     80        0.2832  0.0901\n",
      "     81        \u001b[36m0.2588\u001b[0m  0.0906\n",
      "     82        0.2816  0.0944\n",
      "     83        0.2749  0.0911\n",
      "     84        0.2782  0.0973\n",
      "     85        0.2926  0.0845\n",
      "     86        \u001b[36m0.2404\u001b[0m  0.1030\n",
      "     87        0.2699  0.0929\n",
      "     88        0.2630  0.0908\n",
      "     89        0.2740  0.0905\n",
      "     90        0.2659  0.0919\n",
      "     91        0.2570  0.0911\n",
      "     92        0.2673  0.0894\n",
      "     93        0.2901  0.0948\n",
      "     94        0.2794  0.0887\n",
      "     95        0.2874  0.0895\n",
      "     96        0.2681  0.0926\n",
      "     97        0.2760  0.0905\n",
      "     98        0.2502  0.0914\n",
      "     99        0.2619  0.0910\n",
      "    100        0.2424  0.0915\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.3077\u001b[0m  0.0854\n",
      "      2       10.3077  0.0988\n",
      "      3       10.3077  0.0823\n",
      "      4       10.3077  0.0845\n",
      "      5       10.3077  0.0876\n",
      "      6       10.3077  0.0864\n",
      "      7       10.3077  0.0907\n",
      "      8       10.3077  0.0888\n",
      "      9       10.3077  0.0902\n",
      "     10       10.3077  0.0851\n",
      "     11       10.3077  0.0872\n",
      "     12       10.3077  0.0866\n",
      "     13       10.3077  0.0942\n",
      "     14       10.3077  0.0868\n",
      "     15       10.3077  0.0885\n",
      "     16       10.3077  0.0876\n",
      "     17       10.3077  0.1070\n",
      "     18       10.3077  0.0862\n",
      "     19       10.3077  0.0894\n",
      "     20       10.3077  0.0839\n",
      "     21       10.3077  0.0810\n",
      "     22       10.3077  0.0863\n",
      "     23       10.3077  0.0899\n",
      "     24       10.3077  0.0858\n",
      "     25       10.3077  0.0897\n",
      "     26       10.3077  0.0889\n",
      "     27       10.3077  0.0858\n",
      "     28       10.3077  0.0868\n",
      "     29       10.3077  0.0878\n",
      "     30       10.3077  0.0869\n",
      "     31       10.3077  0.0865\n",
      "     32        \u001b[36m4.3303\u001b[0m  0.0938\n",
      "     33        \u001b[36m0.6218\u001b[0m  0.0883\n",
      "     34        \u001b[36m0.6176\u001b[0m  0.0938\n",
      "     35        0.6256  0.0872\n",
      "     36        0.6177  0.0829\n",
      "     37        \u001b[36m0.5919\u001b[0m  0.0868\n",
      "     38        0.5962  0.0809\n",
      "     39        \u001b[36m0.5843\u001b[0m  0.0837\n",
      "     40        0.6008  0.0887\n",
      "     41        0.5921  0.0853\n",
      "     42        0.5893  0.0889\n",
      "     43        0.5892  0.0829\n",
      "     44        \u001b[36m0.5812\u001b[0m  0.0938\n",
      "     45        \u001b[36m0.5791\u001b[0m  0.0955\n",
      "     46        0.5999  0.0902\n",
      "     47        \u001b[36m0.5776\u001b[0m  0.0859\n",
      "     48        0.5809  0.0883\n",
      "     49        \u001b[36m0.5761\u001b[0m  0.0856\n",
      "     50        0.5770  0.1054\n",
      "     51        \u001b[36m0.5727\u001b[0m  0.0894\n",
      "     52        0.5837  0.0834\n",
      "     53        \u001b[36m0.5662\u001b[0m  0.0903\n",
      "     54        0.5930  0.0953\n",
      "     55        0.5790  0.0824\n",
      "     56        0.5868  0.0891\n",
      "     57        0.5972  0.0942\n",
      "     58        0.5820  0.0899\n",
      "     59        \u001b[36m0.5552\u001b[0m  0.0906\n",
      "     60        0.5946  0.0906\n",
      "     61        0.5841  0.0939\n",
      "     62        0.5751  0.0906\n",
      "     63        0.5925  0.0871\n",
      "     64        0.5932  0.0866\n",
      "     65        0.5980  0.0909\n",
      "     66        0.5768  0.0900\n",
      "     67        0.5834  0.0921\n",
      "     68        0.5849  0.0907\n",
      "     69        0.5918  0.0947\n",
      "     70        0.5878  0.0924\n",
      "     71        0.5794  0.0902\n",
      "     72        0.5857  0.0891\n",
      "     73        0.5786  0.1128\n",
      "     74        0.5804  0.0880\n",
      "     75        0.5868  0.0854\n",
      "     76        0.5650  0.0900\n",
      "     77        0.5988  0.0905\n",
      "     78        0.5666  0.0910\n",
      "     79        0.5875  0.0901\n",
      "     80        0.5676  0.0884\n",
      "     81        0.5682  0.0870\n",
      "     82        0.5841  0.0969\n",
      "     83        0.5770  0.0848\n",
      "     84        0.5733  0.0869\n",
      "     85        0.5917  0.1041\n",
      "     86        0.5767  0.0987\n",
      "     87        0.5825  0.0920\n",
      "     88        0.5763  0.0907\n",
      "     89        0.5797  0.0870\n",
      "     90        0.5722  0.0902\n",
      "     91        0.5834  0.0880\n",
      "     92        0.5725  0.0863\n",
      "     93        0.5786  0.0888\n",
      "     94        0.5899  0.0909\n",
      "     95        0.5732  0.0928\n",
      "     96        0.5666  0.0921\n",
      "     97        0.5789  0.0953\n",
      "     98        0.5668  0.0910\n",
      "     99        0.5909  0.0966\n",
      "    100        0.5768  0.0893\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m10.2876\u001b[0m  0.0851\n",
      "      2       10.2876  0.0868\n",
      "      3       10.2876  0.0869\n",
      "      4       10.2876  0.0877\n",
      "      5       10.2876  0.0882\n",
      "      6       10.2876  0.0872\n",
      "      7       10.2876  0.0902\n",
      "      8       10.2876  0.0887\n",
      "      9       10.2876  0.0850\n",
      "     10       10.2876  0.0899\n",
      "     11       10.2876  0.0863\n",
      "     12       10.2876  0.0917\n",
      "     13       10.2876  0.0866\n",
      "     14       10.2876  0.0843\n",
      "     15       10.2876  0.0861\n",
      "     16       10.2876  0.0915\n",
      "     17       10.2876  0.0888\n",
      "     18       10.2876  0.0898\n",
      "     19       10.2876  0.0847\n",
      "     20       10.2876  0.0850\n",
      "     21       10.2876  0.0950\n",
      "     22       10.2876  0.0863\n",
      "     23       10.2876  0.0891\n",
      "     24       10.2876  0.0903\n",
      "     25        \u001b[36m6.1255\u001b[0m  0.0879\n",
      "     26        \u001b[36m0.6386\u001b[0m  0.0844\n",
      "     27        \u001b[36m0.6002\u001b[0m  0.0924\n",
      "     28        \u001b[36m0.5862\u001b[0m  0.0868\n",
      "     29        \u001b[36m0.5491\u001b[0m  0.0923\n",
      "     30        0.5522  0.0913\n",
      "     31        \u001b[36m0.5459\u001b[0m  0.0923\n",
      "     32        \u001b[36m0.5457\u001b[0m  0.0880\n",
      "     33        \u001b[36m0.5315\u001b[0m  0.0867\n",
      "     34        \u001b[36m0.5309\u001b[0m  0.0873\n",
      "     35        \u001b[36m0.5215\u001b[0m  0.0854\n",
      "     36        \u001b[36m0.5114\u001b[0m  0.0971\n",
      "     37        \u001b[36m0.4793\u001b[0m  0.0860\n",
      "     38        0.4890  0.0820\n",
      "     39        0.4844  0.0880\n",
      "     40        0.4968  0.0906\n",
      "     41        \u001b[36m0.4661\u001b[0m  0.0960\n",
      "     42        \u001b[36m0.4402\u001b[0m  0.0896\n",
      "     43        \u001b[36m0.4211\u001b[0m  0.0839\n",
      "     44        \u001b[36m0.4000\u001b[0m  0.0888\n",
      "     45        0.4124  0.0898\n",
      "     46        \u001b[36m0.3804\u001b[0m  0.0913\n",
      "     47        \u001b[36m0.3540\u001b[0m  0.0880\n",
      "     48        0.3588  0.0964\n",
      "     49        \u001b[36m0.3489\u001b[0m  0.0908\n",
      "     50        \u001b[36m0.3384\u001b[0m  0.0907\n",
      "     51        0.3473  0.0970\n",
      "     52        \u001b[36m0.3129\u001b[0m  0.0868\n",
      "     53        0.3231  0.0855\n",
      "     54        \u001b[36m0.3014\u001b[0m  0.0862\n",
      "     55        0.4398  0.0879\n",
      "     56        0.3173  0.0853\n",
      "     57        \u001b[36m0.2867\u001b[0m  0.0889\n",
      "     58        0.2957  0.0916\n",
      "     59        0.2978  0.0875\n",
      "     60        0.3062  0.0915\n",
      "     61        0.3020  0.0928\n",
      "     62        0.2917  0.0897\n",
      "     63        0.2891  0.0907\n",
      "     64        \u001b[36m0.2792\u001b[0m  0.0885\n",
      "     65        \u001b[36m0.2784\u001b[0m  0.0893\n",
      "     66        \u001b[36m0.2561\u001b[0m  0.0899\n",
      "     67        \u001b[36m0.2373\u001b[0m  0.0912\n",
      "     68        0.2453  0.0902\n",
      "     69        0.2518  0.0894\n",
      "     70        0.2505  0.0968\n",
      "     71        0.2751  0.1025\n",
      "     72        0.2642  0.0964\n",
      "     73        0.2437  0.0928\n",
      "     74        0.2556  0.0909\n",
      "     75        0.2579  0.0898\n",
      "     76        0.2551  0.0918\n",
      "     77        0.2468  0.0921\n",
      "     78        \u001b[36m0.2339\u001b[0m  0.0917\n",
      "     79        \u001b[36m0.2220\u001b[0m  0.0938\n",
      "     80        0.2676  0.0871\n",
      "     81        0.2658  0.0859\n",
      "     82        0.2428  0.0865\n",
      "     83        0.2597  0.0875\n",
      "     84        0.2474  0.0934\n",
      "     85        0.2664  0.0935\n",
      "     86        0.2590  0.0912\n",
      "     87        0.2590  0.0947\n",
      "     88        \u001b[36m0.2166\u001b[0m  0.1019\n",
      "     89        0.2471  0.0910\n",
      "     90        0.2622  0.0901\n",
      "     91        0.2699  0.0896\n",
      "     92        0.2439  0.0889\n",
      "     93        0.2438  0.0909\n",
      "     94        0.2347  0.0910\n",
      "     95        0.2220  0.0923\n",
      "     96        0.2310  0.0906\n",
      "     97        0.2325  0.0940\n",
      "     98        0.2356  0.0878\n",
      "     99        0.2295  0.0939\n",
      "    100        0.2431  0.0928\n"
     ]
    }
   ],
   "source": [
    "resultados = cross_val_score(classificador_sklearn, previsores, classe, cv = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xUhUTS4vNTvD",
    "outputId": "d322b0be-49eb-4025-889b-98f1aa03829a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.838408521303258, 0.1024364042134692)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media = resultados.mean()\n",
    "desvio = resultados.std()\n",
    "media, desvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "aNUs81ZJNfvb",
    "outputId": "7e605996-f55f-4dc0-f762-4fb47a4f301d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84210526, 0.85964912, 0.89473684, 0.92982456, 0.85964912,\n",
       "       0.9122807 , 0.64912281, 0.9122807 , 0.63157895, 0.89285714])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Projeto #2 Classificação binária breast cancer com validação cruzada.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbaseconda5f7fb1aed7e442218f6533dd9bf3a4e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
