{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecção de Emoções em Videos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6RvV2dC_IeG"
      },
      "source": [
        "# Detecção de Emoções em Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWPgmYF0YWuc"
      },
      "source": [
        "## Etapa 1 - Importando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTjPR159Rxid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55ab8b4c-0325-49ba-f6d7-6ea2e803e04c"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "\n",
        "cv2.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI_stTb4ja_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ca5435e2-f376-4e87-cfab-2b2f3ad7ab8c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbhivnG9Y8Zb"
      },
      "source": [
        "## Etapa 2 - Conectando com o Drive e acessando os arquivos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgetMW9FNmuF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83df2f82-5eb7-4921-a00d-925f3eae8ca9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB_J0KIRZEdW"
      },
      "source": [
        "path = \"/content/gdrive/My Drive/Videos.zip\"\n",
        "zip_object = zipfile.ZipFile(file=path, mode=\"r\")\n",
        "zip_object.extractall(\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drgE_XHlK5HZ"
      },
      "source": [
        "## Etapa 3 - Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5CRLoHOMh3z"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "diretorio = 'gdrive/My Drive/Cursos/Deteccao_Expressoes_Faciais/' # diretorio do drive onde estão os arquivos do curso\n",
        "\n",
        "# vamos utilizar o Modelo da Arquitetura 2 pois foi o que se saiu melhor em nossos testes\n",
        "model = load_model(diretorio + \"modelo_02_expressoes.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDTvpCWoMlSD"
      },
      "source": [
        "## Etapa 4 - Carregando o vídeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYadz-xeqGUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43ab15aa-6fdb-4336-a95a-fffdbb5648d4"
      },
      "source": [
        "arquivo_video = diretorio + \"testes/video_teste04.mp4\"\n",
        "cap = cv2.VideoCapture(arquivo_video)\n",
        "\n",
        "conectado, video = cap.read()\n",
        "print(video.shape) # mostra as dimensões do video"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(360, 640, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKNKjJC2NKC8"
      },
      "source": [
        "## Etapa 5 - Redimensionando o tamanho (opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_o8KycHwq44"
      },
      "source": [
        "\n",
        "Recomendado quando o tamanho do vídeo é muito grande. \n",
        "Se o vídeo tiver a resolução muito alta então pode demorar muito o processamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyAWWB-9wmzs"
      },
      "source": [
        "redimensionar = True \n",
        "# deixe True para reduzir o tamanho do vídeo salvo caso este supere a largura máxima que vamos especificar abaixo.\n",
        "# para manter o tamanho original deixe False \n",
        "largura_maxima = 600  # pixels. define o tamanho da largura (máxima) do vídeo a ser salvo. a altura será proporcional e é definida nos calculos abaixo\n",
        "\n",
        "# se redimensionar = True então o video que será salvo terá seu tamanho em pixels reduzido SE for maior que a largura_maxima\n",
        "\n",
        "if (redimensionar and video.shape[1]>largura_maxima):\n",
        "  # precisamos deixar a largura e altura proporcionais (mantendo a proporção do vídeo original) para que a imagem não fique com aparência esticada\n",
        "  proporcao = video.shape[1] / video.shape[0]\n",
        "  # para isso devemos calcular a proporção (largura/altura) e usaremos esse valor para calcular a altura (com base na largura que definimos acima) \n",
        "  video_largura = largura_maxima\n",
        "  video_altura = int(video_largura / proporcao)\n",
        "else:\n",
        "  video_largura = video.shape[1]\n",
        "  video_altura = video.shape[0]\n",
        "\n",
        "# se redimensionar = False então os valores da largura e altura permanecerão os mesmos do vídeo original  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5mjohEn5ES9"
      },
      "source": [
        "## Etapa 6 - Definindo as configurações do vídeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-wF747A4_91"
      },
      "source": [
        "# nome do arquivo de vídeo que será salvo\n",
        "nome_arquivo = diretorio+'resultado_video_teste04.avi' \n",
        "\n",
        "# definição do codec\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
        "# FourCC é um código de 4 bytes usado para especificar o codec de vídeo. A lista de códigos disponíveis pode ser encontrada no site fourcc.org\n",
        "# Codecs mais usados: XVID, MP4V, MJPG, DIVX, X264... \n",
        "# Por exemplo, para salvar em formato mp4 utiliza-se o codec mp4v (o nome do arquivo também precisa possuir a extensão .mp4)\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
        "\n",
        "# FPS - frames por segundo\n",
        "fps = 24\n",
        "# se quiser deixar o video um pouco mais lento pode diminuir o numero de frames por segundo para 20\n",
        "\n",
        "saida_video = cv2.VideoWriter(nome_arquivo, fourcc, fps, (video_largura, video_altura))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nSQGa1Owqlv"
      },
      "source": [
        "* Mais exemplos de outras configurações com o fourcc que é possível usar: https://www.programcreek.com/python/example/89348/cv2.VideoWriter_fourcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f9neVe19jOZ"
      },
      "source": [
        "## Etapa 7 - Processamento do vídeo e gravação do resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYGv1W-Lpywi"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "haarcascade_faces = diretorio + 'haarcascade_frontalface_alt.xml' # arquivo haarcascade\n",
        "\n",
        "# define os tamanhos para as fontes\n",
        "fonte_pequena, fonte_media = 0.4, 0.7\n",
        "\n",
        "fonte = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "expressoes = [\"Raiva\", \"Nojo\", \"Medo\", \"Feliz\", \"Triste\", \"Surpreso\", \"Neutro\"] \n",
        "\n",
        "while (cv2.waitKey(1) < 0):\n",
        "    conectado, frame = cap.read()\n",
        "    \n",
        "    if not conectado:\n",
        "        break  # se ocorreu um problema ao carregar a imagem então interrompe o programa\n",
        "\n",
        "    t = time.time() # tempo atual, antes de iniciar (vamos utilizar para calcular quanto tempo levou para executar as operações)\n",
        "    \n",
        "    # frame_video = np.copy(frame) # faz uma copia do frame do video\n",
        "\n",
        "    if redimensionar: # se redimensionar = True então redimensiona o frame para os novos tamanhos\n",
        "      frame = cv2.resize(frame, (video_largura, video_altura)) \n",
        "\n",
        "    face_cascade = cv2.CascadeClassifier(haarcascade_faces)\n",
        "    cinza = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # converte pra grayscale\n",
        "    faces = face_cascade.detectMultiScale(cinza,scaleFactor=1.2, minNeighbors=5,minSize=(30,30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        for (x, y, w, h) in faces:\n",
        "\n",
        "            frame = cv2.rectangle(frame,(x,y),(x+w,y+h+10),(255,50,50),2) # desenha retângulo ao redor da face\n",
        "\n",
        "            roi = cinza[y:y + h, x:x + w]      # extrai apenas a região de interesse (ROI) que é onde contém o rosto \n",
        "            roi = cv2.resize(roi, (48, 48))    # antes de passar pra rede neural redimensiona para o tamanho das imagens de treinamento\n",
        "            roi = roi.astype(\"float\") / 255.0  # normaliza\n",
        "            roi = img_to_array(roi)            # converte para array para que a rede possa processar\n",
        "            roi = np.expand_dims(roi, axis=0)  # muda o shape da array\n",
        "\n",
        "            # faz a predição - calcula as probabilidades\n",
        "            result = model.predict(roi)[0]\n",
        "            print(result)\n",
        "                \n",
        "            if result is not None:\n",
        "                resultado = np.argmax(result) # encontra a emoção com maior probabilidade\n",
        "                cv2.putText(frame,expressoes[resultado],(x,y-10), fonte, fonte_media,(255,255,255),1,cv2.LINE_AA) # escreve a emoção acima do rosto\n",
        "\n",
        "    # tempo processado = tempo atual (time.time()) - tempo inicial (t)\n",
        "    cv2.putText(frame, \" frame processado em {:.2f} segundos\".format(time.time() - t), (20, video_altura-20), fonte, fonte_pequena, (250, 250, 250), 0, lineType=cv2.LINE_AA)\n",
        "\n",
        "    cv2_imshow(frame) \n",
        "    saida_video.write(frame) # grava o frame atual\n",
        "\n",
        "print(\"Terminou\")\n",
        "saida_video.release() \n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}