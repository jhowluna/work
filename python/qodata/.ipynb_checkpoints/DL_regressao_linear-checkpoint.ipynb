{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47NbPBLo_OU7"
   },
   "source": [
    "#Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:56.533410Z",
     "start_time": "2020-07-23T14:15:55.425106Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WEhH_vMxvxRU"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = scale(boston.data), boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:57.776603Z",
     "start_time": "2020-07-23T14:15:56.636356Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dnkBVR1Kv82X",
    "outputId": "da4ad053-43ae-4a0a-b91e-54baa57976e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "regression = LinearRegression() \n",
    "regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:57.952595Z",
     "start_time": "2020-07-23T14:15:57.900618Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k5FNKD8rwEIA",
    "outputId": "8fd6eaf5-3f6f-49bd-8639-018b89bbad6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.741\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Agora que o algoritmo está instalado, você pode usar o método de \n",
    "pontuação para relatar a medida R2:\n",
    "\"\"\"\n",
    "print(f'R2 {regression.score(X, y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:58.063602Z",
     "start_time": "2020-07-23T14:15:58.051595Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "0nEpshmFzcYC",
    "outputId": "7667dbfb-5042-4ab1-fdcb-00cfcc37f672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM:-0.9\n",
      "ZN:1.1\n",
      "INDUS:0.1\n",
      "CHAS:0.7\n",
      "NOX:-2.1\n",
      "RM:2.7\n",
      "AGE:0.0\n",
      "DIS:-3.1\n",
      "RAD:2.7\n",
      "TAX:-2.1\n",
      "PTRATIO:-2.1\n",
      "B:0.8\n",
      "LSTAT:-3.7\n"
     ]
    }
   ],
   "source": [
    "elementos = [a + ':' + str(round(b, 1)) for a, b in zip(boston.feature_names, regression.coef_)]\n",
    "\n",
    "for elemento in elementos:\n",
    "  print(elemento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:58.173600Z",
     "start_time": "2020-07-23T14:15:58.163602Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "dMnk6g8MwqOk",
    "outputId": "d71c90c5-cec0-4dd1-9abb-fefd6582eb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:58.331598Z",
     "start_time": "2020-07-23T14:15:58.255597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "xFj4GJko2Dcf",
    "outputId": "37bfda8d-d840-4283-ba3b-5ec2b40d30a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "Vermelho = 2\n",
      "Verde = 1\n",
      "Azul = 0\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODER VS LABEL ENCODER\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl = LabelEncoder()\n",
    "enc = OneHotEncoder()\n",
    "qualitativo = ['vermelho', 'vermelho', 'verde', 'azul',\n",
    " 'vermelho', 'azul', 'azul', 'verde']\n",
    "labels = lbl.fit_transform(qualitativo).reshape(8,1)\n",
    "print(labels)\n",
    "print()\n",
    "print(\"Vermelho = 2\\nVerde = 1\\nAzul = 0\")\n",
    "print()\n",
    "print(enc.fit_transform(labels).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:58.474598Z",
     "start_time": "2020-07-23T14:15:58.448597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "urH-2yTo_0VH",
    "outputId": "651927c6-7919-4b2a-caa4-3bc7370b82f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PolynomialFeatures in module sklearn.preprocessing._data:\n",
      "\n",
      "class PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      " |  \n",
      " |  Generate polynomial and interaction features.\n",
      " |  \n",
      " |  Generate a new feature matrix consisting of all polynomial combinations\n",
      " |  of the features with degree less than or equal to the specified degree.\n",
      " |  For example, if an input sample is two dimensional and of the form\n",
      " |  [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  degree : integer\n",
      " |      The degree of the polynomial features. Default = 2.\n",
      " |  \n",
      " |  interaction_only : boolean, default = False\n",
      " |      If true, only interaction features are produced: features that are\n",
      " |      products of at most ``degree`` *distinct* input features (so not\n",
      " |      ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n",
      " |  \n",
      " |  include_bias : boolean\n",
      " |      If True (default), then include a bias column, the feature in which\n",
      " |      all polynomial powers are zero (i.e. a column of ones - acts as an\n",
      " |      intercept term in a linear model).\n",
      " |  \n",
      " |  order : str in {'C', 'F'}, default 'C'\n",
      " |      Order of output array in the dense case. 'F' order is faster to\n",
      " |      compute, but may slow down subsequent estimators.\n",
      " |  \n",
      " |      .. versionadded:: 0.21\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.preprocessing import PolynomialFeatures\n",
      " |  >>> X = np.arange(6).reshape(3, 2)\n",
      " |  >>> X\n",
      " |  array([[0, 1],\n",
      " |         [2, 3],\n",
      " |         [4, 5]])\n",
      " |  >>> poly = PolynomialFeatures(2)\n",
      " |  >>> poly.fit_transform(X)\n",
      " |  array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
      " |         [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
      " |         [ 1.,  4.,  5., 16., 20., 25.]])\n",
      " |  >>> poly = PolynomialFeatures(interaction_only=True)\n",
      " |  >>> poly.fit_transform(X)\n",
      " |  array([[ 1.,  0.,  1.,  0.],\n",
      " |         [ 1.,  2.,  3.,  6.],\n",
      " |         [ 1.,  4.,  5., 20.]])\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  powers_ : array, shape (n_output_features, n_input_features)\n",
      " |      powers_[i, j] is the exponent of the jth input in the ith output.\n",
      " |  \n",
      " |  n_input_features_ : int\n",
      " |      The total number of input features.\n",
      " |  \n",
      " |  n_output_features_ : int\n",
      " |      The total number of polynomial output features. The number of output\n",
      " |      features is computed by iterating over all suitably sized combinations\n",
      " |      of input features.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Be aware that the number of features in the output array scales\n",
      " |  polynomially in the number of features of the input array, and\n",
      " |  exponentially in the degree. High degrees can cause overfitting.\n",
      " |  \n",
      " |  See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
      " |  <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PolynomialFeatures\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute number of output features.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : instance\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of string, length n_features, optional\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : list of string, length n_output_features\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform data to polynomial features\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or CSR/CSC sparse matrix, shape [n_samples, n_features]\n",
      " |          The data to transform, row by row.\n",
      " |      \n",
      " |          Prefer CSR over CSC for sparse input (for speed), but CSC is\n",
      " |          required if the degree is 4 or higher. If the degree is less than\n",
      " |          4 and the input format is CSC, it will be converted to CSR, have\n",
      " |          its polynomial features generated, then converted back to CSC.\n",
      " |      \n",
      " |          If the degree is 2 or 3, the method described in \"Leveraging\n",
      " |          Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n",
      " |          Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n",
      " |          used, which is much faster than the method used on CSC input. For\n",
      " |          this reason, a CSC input will be converted to CSR, and the output\n",
      " |          will be converted back to CSC prior to being returned, hence the\n",
      " |          preference of CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      XP : np.ndarray or CSR/CSC sparse matrix, shape [n_samples, NP]\n",
      " |          The matrix of features, where NP is the number of polynomial\n",
      " |          features generated from the combination of inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  powers_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "help(PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:58.858599Z",
     "start_time": "2020-07-23T14:15:58.575597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OjrI6QfQ91rQ",
    "outputId": "42f04d1d-6647-424d-b5e3-33381f3558d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.820\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "O exemplo python a seguir usa o conjunto de dados de Boston para verificar \n",
    "a eficácia da técnica. Se for bem-sucedida, a expansão polinômial pegará \n",
    "relações não lineares em dados que requerem uma curva, não uma linha, \n",
    "para prever corretamente e superar qualquer dificuldade de previsão em \n",
    "detrimento de um número crescente de preditores.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "Polinomial = PolynomialFeatures(degree=2)\n",
    "Polinomial_X = Polinomial.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Polinomial_X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "regressao = Ridge(alpha=0.1, normalize=True)\n",
    "regressao.fit(X_train,y_train)\n",
    "print(f'R2: {r2_score(y_test,regressao.predict(X_test)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:59.050592Z",
     "start_time": "2020-07-23T14:15:59.005599Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5_FXQ1JxFCFH",
    "outputId": "14dd9ced-b50b-4359-f22d-b117f128549f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Uma solução para um problema envolvendo uma resposta binária\n",
    "(o modelo tem que escolher entre duas classes possíveis) \n",
    "seria codificar um vetor de resposta como uma sequência de uns e zeros \n",
    "(ou valores positivos e negativos). \n",
    "O seguinte código Python prova a viabilidade e os limites do uso\n",
    "de uma resposta binária:\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "a = np.array([0, 0, 0, 0, 1, 1, 1, 1]) \n",
    "b = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(8,1) \n",
    "from sklearn.linear_model import LinearRegression \n",
    "regressao = LinearRegression() \n",
    "regressao.fit(b,a) \n",
    "print (regressao.predict(b)>0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:59.464604Z",
     "start_time": "2020-07-23T14:15:59.201597Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Vw5mF47BLnj6",
    "outputId": "0b07d7b7-e19d-4a80-d520-6f539a98bacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento: 0.979\n",
      "Teste:0.958\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A regressão logística é a mesma que uma regressão linear, \n",
    "exceto que os dados y contêm números inteiros indicando a classe \n",
    "em relação à observação. Assim, usando o conjunto de dados de Boston \n",
    "a partir do módulo de conjuntos de dados Scikit-learn, você pode tentar \n",
    "adivinhar o que torna as casas em uma área excessivamente cara\n",
    "(valores medianos >= 40):\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "binary_y = np.array(y >= 40).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,binary_y, test_size=0.33, random_state=5)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train,y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Treinamento: {accuracy_score(y_train, logistic.predict(X_train)):.3f}')\n",
    "print(f'Teste:{accuracy_score(y_test, logistic.predict(X_test)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:15:59.798734Z",
     "start_time": "2020-07-23T14:15:59.784730Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "gNOyjxFGMK2B",
    "outputId": "d9255aab-7926-42d8-8630-ec7c1b7bf02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM : 0.086\n",
      "ZN : 0.230\n",
      "INDUS : 0.580\n",
      "CHAS : -0.029\n",
      "NOX : -0.304\n",
      "RM : 1.769\n",
      "AGE : -0.127\n",
      "DIS : -0.539\n",
      "RAD : 0.919\n",
      "TAX : -0.165\n",
      "PTRATIO : -0.782\n",
      "B : 0.077\n",
      "LSTAT : -1.628\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "O exemplo divide os dados em conjuntos de treinamento e teste, permitindo \n",
    "verificar a eficácia do modelo de regressão logística em dados que o modelo \n",
    "não usou para aprender. Os coeficientes resultantes dizem a probabilidade de uma\n",
    " determinada classe estar na classe-alvo\n",
    " (que é qualquer classe codificada usando um valor de 1). \n",
    " Se um coeficiente aumentar a probabilidade, terá um coeficiente positivo; \n",
    " caso contrário, o coeficiente é negativo.\n",
    "\"\"\"\n",
    "for var,coef in zip(boston.feature_names,\n",
    " logistic.coef_[0]):\n",
    " print (f\"{var} : {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:16:00.210353Z",
     "start_time": "2020-07-23T14:16:00.202351Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "7f7lC1WEM2u-",
    "outputId": "a8ded24e-4d1a-402a-c337-ef29d4bb0b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classes: [0 1]\n",
      "\n",
      "Probs:\n",
      " [[0.33234217 0.66765783]\n",
      " [0.97060356 0.02939644]\n",
      " [0.99594746 0.00405254]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lendo os resultados na sua tela, você pode ver que em Boston, a criminalidade (CRIM)\n",
    " tem algum efeito sobre os preços. No entanto, o nível de pobreza (LSTAT), \n",
    " a distância do trabalho (DIS) e a poluição (NOX) têm efeitos muito maiores. \n",
    " Além disso, ao contrário da regressão linear, a regressão logística não produz \n",
    " simplesmente a classe resultante (neste caso um 1 ou um 0), mas também estima a \n",
    " probabilidade de a observação fazer parte de uma das duas classes:\n",
    "\"\"\"\n",
    "print('\\nclasses:',logistic.classes_)\n",
    "print('\\nProbs:\\n',logistic.predict_proba(X_test)[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:16:01.716535Z",
     "start_time": "2020-07-23T14:16:00.675679Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ZqjZ28cmN7wm",
    "outputId": "82decca1-7356-40b9-ff69-deacefc9a5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random features: 1 -> R2: 0.739\n",
      "Random features: 2 -> R2: 0.740\n",
      "Random features: 4 -> R2: 0.740\n",
      "Random features: 8 -> R2: 0.740\n",
      "Random features: 16 -> R2: 0.748\n",
      "Random features: 32 -> R2: 0.754\n",
      "Random features: 64 -> R2: 0.773\n",
      "Random features: 128 -> R2: 0.824\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A menos que você use validação cruzada, medidas de erro como R2 podem ser enganosas\n",
    "porque o número de features pode facilmente inflar, mesmo que a feature não contenha \n",
    "informações relevantes. O exemplo a seguir mostra o que acontece com o R2 quando você \n",
    "adiciona apenas features aleatórias:\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "check = [2**i for i in range(8)]\n",
    "\n",
    "for i in range(2**7+1):\n",
    "  X_train = np.column_stack((X_train,np.random.random(X_train.shape[0])))\n",
    "  X_test = np.column_stack((X_test,np.random.random(X_test.shape[0])))\n",
    "  regressao.fit(X_train, y_train)\n",
    "  \n",
    "  if i in check:\n",
    "    print (f\"Random features: {i} -> R2: {r2_score(y_train,regressao.predict(X_train)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:16:02.162542Z",
     "start_time": "2020-07-23T14:16:01.865536Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1nKRRCbMPBYv",
    "outputId": "9dcec4e6-07b3-4d29-ad57-805b427aa530"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 13 is different from 142)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d0f76bbc7a40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mregressao\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'R2 {r2_score(y_test,regression.predict(X_test)):.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 220\u001b[1;33m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 13 is different from 142)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "O que parece ser uma capacidade preditiva aumentada é apenas uma ilusão. Você pode revelar o que aconteceu verificando o conjunto de testes e descobrindo que o desempenho do modelo diminuiu:\n",
    "a)Observe que o resultado R2 pode mudar de execução\n",
    "b)devido à natureza aleatória do experimento R2 0.474\n",
    "\"\"\"\n",
    "\n",
    "regressao.fit(X_train, y_train)\n",
    "print(f'R2 {r2_score(y_test,regression.predict(X_test)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:16:02.542367Z",
     "start_time": "2020-07-23T14:16:02.514372Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8KJgeB7hRtUV",
    "outputId": "814f107d-a6ea-4e9a-b357-a2ae2b78a254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.820\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "O exemplo a seguir modifica o exemplo de expansões polinomiais \n",
    "usando a regularização L2 e reduz a influência de coeficientes redundantes \n",
    "criados pelo procedimento de expansão:\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "poly_X = pf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "reg_regression = Ridge(alpha=0.1, normalize=True)\n",
    "reg_regression.fit(X_train,y_train)\n",
    "print (f'R2: {r2_score(y_test,reg_regression.predict(X_test)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T18:48:10.993084Z",
     "start_time": "2020-07-22T18:48:05.605Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "BqnH_93F3atM",
    "outputId": "e2e40386-804a-43c6-f6e5-5075ce1fb895"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Para demonstrar a eficácia do aprendizado fora do núcleo, \n",
    "o exemplo a seguir estabelece um breve experimento em Python usando \n",
    "regressão e squared_loss como função de custo. Ele se baseia no \n",
    "dataset Boston depois de embaralhá-lo e separá-lo em conjuntos de \n",
    "treinamento e testes. O exemplo demonstra como os coeficientes beta \n",
    "mudam à medida que o algoritmo vê mais exemplos.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "SGD = SGDRegressor(penalty=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, max_iter=5, tol=None)\n",
    "power = 17\n",
    "check = [2**i for i in range(power+1)]\n",
    "\n",
    "for i in range(400):\n",
    " for j in range(X_train.shape[0]):\n",
    "  SGD.partial_fit(X_train[j,:].reshape(1,13), y_train[j].reshape(1,))\n",
    "  count = (j+1) + X_train.shape[0] * i\n",
    "  if count in check:\n",
    "    R2 = r2_score(y_test,SGD.predict(X_test))\n",
    "    print (f'Exemplo {count} R2 {R2:.3f} coef:'+' '.join(map( lambda x:'%0.3f' %x, SGD.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qi2TfvO5rIX"
   },
   "source": [
    "Lembre-se: Não importa a quantidade de dados, você sempre pode se encaixar em um modelo de regressão linear simples, mas eficaz, usando features de aprendizagem on-line SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZXrHjU650g2"
   },
   "source": [
    "# ATIVIDADE PRÁTICA: Linear/Polynomial regression\n",
    "Com o dataset carregado, crie:\n",
    "1. Modelo preditivo utilizando Regressão Linear(utilize dataviz+scores como R2)\n",
    "2. Modelo preditivo utilizando Regressão Polinomial(utilize dataviz+scores como R2)\n",
    "<br><br>Ao término, através do score R2, comprove cientificamente qual melhor algoritmo para o dataset compartilhado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "h5EAzgmXnPU1",
    "outputId": "25210c7a-6cd1-4e7f-9ebf-dab3aad05a07"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LIBS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DATASET\n",
    "np.random.seed(0)\n",
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# ADICIONAMOS NOVA COLUNA AO INVÉS DE RESHAPE COM NP.NEWAXIS\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "# DATAVIZ\n",
    "plt.scatter(x,y, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJaM7iZU-fCF"
   },
   "outputs": [],
   "source": [
    "# 1 LINEAR REGRESSION\n",
    "\"\"\"\n",
    "Dicas modelo preditivo: \n",
    "a)importe seu algoritmo Linear Regression, \n",
    "b)instancie ele em uma variável, \n",
    "c)utilize .fit() para treinar\n",
    "d) instancie uma variável para .predict()\n",
    "e) instancie RMSE e R2\n",
    "\n",
    "Dicas DataViz:\n",
    "a) Utilize .scatter()\n",
    "b) Utilize .plot()\n",
    "c) utilize .show()\n",
    "\n",
    "Visite as documentações\n",
    "\"\"\"\n",
    "# importações essenciais:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwS02c6J-g8r"
   },
   "outputs": [],
   "source": [
    "# 2 POLYNOMIAL REGRESSION\n",
    "\"\"\"\n",
    "a) instancie a PolynomialFeatures em uma variável \n",
    "b) nesta variável, use .fit_transform() para trata-la \n",
    "c) instancie seu algoritmo de Regressão Linear em uma variável\n",
    "d) use o .fit() para treina-lo \n",
    "e) instancie em uma variável o .predict()\n",
    "f) instancie RMSE e R2\n",
    "g) utilize .scatter(), .plot() e .show() para dataviz\n",
    "\n",
    "Visite as documentações\n",
    "\"\"\"\n",
    "\n",
    "# importações essenciais:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DL-regressao-linear.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
